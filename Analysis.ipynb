{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries"
      ],
      "metadata": {
        "id": "LLUkGszRbSsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "qr0nHLzObPIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUz9seM3OzSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminary Analysis"
      ],
      "metadata": {
        "id": "AOOP5-v6bWnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./train.csv')\n",
        "train_df2 = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "Ah0SZu9Abkr-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "e4678ed3-b59b-43f0-89a6-44915347804c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4f62658a2a32>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train_df = pd.read_csv('./train.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df.info()\n",
        "\n",
        "train_df.describe().T\n",
        "\n",
        "## chooing to drop nan or not should come by looking at the scatter plots/value counts hist of the data\n",
        "#for example nan in alley means no alley which is information not to throw away. make it a 'noAlley' category"
      ],
      "metadata": {
        "id": "RCMKVRB6bVyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MISSING VALUES\n",
        "\n",
        "missing = pd.DataFrame(train_df.isnull().sum().sort_values(ascending=False))\n",
        "missing.columns = ['count']\n",
        "missing = missing.loc[(missing!=0).any(axis=1)]\n",
        "missing['percentage'] = missing[0:]/1460\n",
        "missing.style.background_gradient()\n"
      ],
      "metadata": {
        "id": "zHirpCW4b606"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting numeric and non numeric columns\n",
        "\n",
        "non_numeric_cols = list(train_df.dtypes[train_df.dtypes == 'object'].index)  # Non-numeric\n",
        "numeric_cols = list(train_df.dtypes[train_df.dtypes != 'object'].index)     # Numeric"
      ],
      "metadata": {
        "id": "6_TY3GCUf5s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPUTATION\n",
        "\n",
        "# Numeric features:\n",
        "df['A'] = df['A'].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "df['A'].fillna(method='ffill', inplace = True)\n",
        "df['A'].fillna(0, inplace = True)\n",
        "df['A_interpolated_linear'] = df['A'].interpolate(method='linear')\n",
        "df['A_interpolated_quadratic'] = df['A'].interpolate(method='polynomial', order=2)\n",
        "\n",
        "\n",
        "# Categorical features:\n",
        "imputer = SimpleImputer(strategy='constant', fill_value='Do_not_have_this_feature')\n",
        "df[['feature_1', 'feature_2']] = imputer.fit_transform(df[['feature_1', 'feature_2']])\n",
        "\n",
        "#Ordinal features\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df[['feature_1', 'feature_2']] = imputer.fit_transform(df[['feature_1', 'feature_2']])"
      ],
      "metadata": {
        "id": "qdP2nat1fQVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Time series snippets"
      ],
      "metadata": {
        "id": "NW7ihbvMe7cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if day 1, day 2 etc are columns in the dataframe instead of rows and we need to do time series regression, then use this\n",
        "\n",
        "##melt data\n",
        "\n",
        "df_melt = pd.melt(df, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold')\n",
        "\n",
        "#this will create a dataframe with column called 'd' where all the columns not in id_vars will be in each row and the value_name is the target variable\n"
      ],
      "metadata": {
        "id": "8GXhpOz8dLG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Decompose Time Series\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Decompose the time series into trend, seasonal, and residual components\n",
        "result = seasonal_decompose(df['value_column'], model='additive', period=12)  # Adjust period for data's seasonality\n",
        "\n",
        "# Plot decomposed components\n",
        "result.plot()\n",
        "plt.show()\n",
        "\n",
        "# Extract components into the dataset\n",
        "df['trend'] = result.trend\n",
        "df['seasonal'] = result.seasonal\n",
        "df['residual'] = result.resid\n",
        "\n",
        "# 2. Check for Stationarity (e.g., using Augmented Dickey-Fuller test)\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "adf_test = adfuller(df['value_column'].dropna())\n",
        "print(\"ADF Statistic:\", adf_test[0])\n",
        "print(\"p-value:\", adf_test[1])  # p < 0.05 indicates stationarity\n",
        "\n",
        "# 3. Plot ACF and PACF\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "plt.figure()\n",
        "plot_acf(df['value_column'].dropna(), lags=20)  # Adjust lags based on data\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plot_pacf(df['value_column'].dropna(), lags=20)  # Adjust lags based on data\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1SsrIMxtFDqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory"
      ],
      "metadata": {
        "id": "oIXrvMoAobHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For classification, also do class imbalance analysis in the beginning\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Define SMOTE oversampler\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE\n",
        "X_resampled, y_resampled = smote.fit_resample(data.drop('target', axis=1), data['target'])\n",
        "\n",
        "# Check new class distribution\n",
        "print(\"New Class Distribution:\\n\", pd.Series(y_resampled).value_counts())"
      ],
      "metadata": {
        "id": "2nGVJ_wTalgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#String manipulation\n",
        "data['Cabin'] = data['Cabin'].apply(lambda x: str(x)[0] if pd.notnull(x) else \"Not Defined\")"
      ],
      "metadata": {
        "id": "IuzQ-z3Cc_ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOTTING\n",
        "\n",
        "# 1. Scatter Plot\n",
        "\n",
        "fig, ax = plt.subplots(12, 3, figsize=(23, 60))\n",
        "for var, subplot in zip(numerical_features, ax.flatten()):\n",
        "    sns.scatterplot(x=var, y='SalePrice',  data=train_df, ax=subplot, hue = 'SalePrice')\n",
        "\n",
        "# 2. Pair Plot\n",
        "sns.pairplot(df)\n",
        "plt.title('Pairplot of Variables')\n",
        "plt.show()\n",
        "\n",
        "# 3. Categorical feature plotting\n",
        "sns.catplot(data=train_df, x=\"Alley\", y=\"SalePrice\", kind=\"box\")"
      ],
      "metadata": {
        "id": "jurOWJEyf_9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRELATION ANALYSIS\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "corr_mat = train_df[numerical_features].corr()\n",
        "sns.heatmap(corr_mat, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T9WA1WGIAMWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURES HISTOGRAM\n",
        "\n",
        "for var in numerical_features:\n",
        "    plt.figure(figsize=(8, 4))  # Set figure size\n",
        "    plt.hist(train_df[var].dropna(), bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    plt.title(f\"Histogram of {var}\")\n",
        "    plt.xlabel(var)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Instead of histogram, can also use distplot():\n",
        "\n",
        "sns.distplot(data['Fare'], kde=True, bins=80, color='green')\n",
        "plt.title('Distribution Plot of a Fare of Passengers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "npjPB02lpONV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MUTUAL INFORMATION\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "mutual_info_func = mutual_info_classif if is_classification else mutual_info_regression\n",
        "\n",
        "# Compute mutual information\n",
        "mutual_info = mutual_info_func(X, y, random_state=42)\n",
        "\n",
        "# Organize results into a DataFrame\n",
        "mutual_info_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Mutual Information': mutual_info\n",
        "}).sort_values(by='Mutual Information', ascending=False)"
      ],
      "metadata": {
        "id": "Na0Bde7JG8sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECKING NORMAL DISTRIBUTION\n",
        "\n",
        "# Plotting the distribution\n",
        "sns.histplot(data, kde=True, stat=\"density\", linewidth=0)\n",
        "plt.title('Distribution of value_column')\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Overlay a normal distribution\n",
        "mu, std = norm.fit(data)\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = norm.pdf(x, mu, std)\n",
        "plt.plot(x, p, 'k', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VUwwJ-xWppX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OUTLIER DETECTION\n",
        "\n",
        "## scatter plot feature vs target and manually drop like\n",
        "plt.scatter(x='LotArea', y='SalePrice', data=train_df)\n",
        "plt.show()\n",
        "\n",
        "##robust z score\n",
        "from statsmodels.robust.scale import mad\n",
        "median = train_df[numeric_cols].median()\n",
        "mad_value = mad(train_df[numeric_cols])\n",
        "robust_z_score = 0.6745 * ((train_df[numeric_cols] - median) / mad_value)\n",
        "threshold = 3\n",
        "df_robust_z = train_df[numeric_cols][~(np.abs(robust_z_score) > threshold).any(axis=1)]\n",
        "\n",
        "###regular z-score\n",
        "stats.zscore(train_df['LotArea']).sort_values().tail(10)\n",
        "\n",
        "###for categorical\n",
        "plt.scatter(x='OverallQual', y='SalePrice', data=train_df)\n",
        "plt.show()\n",
        "# train_df.query('OverallCond == 10 & SalePrice < 200000') remove like this\n",
        "\n",
        "##IQR\n",
        "# Q1 = df.quantile(0.25)\n",
        "# Q3 = df.quantile(0.75)\n",
        "# IQR = Q3 - Q1\n",
        "# df_iqr = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "\n",
        "##Percentile\n",
        "\n",
        "lower_bound = df['feature'].quantile(0.01)\n",
        "upper_bound = df['feature'].quantile(0.99)\n",
        "outliers = df[(df['feature'] < lower_bound) | (df['feature'] > upper_bound)]"
      ],
      "metadata": {
        "id": "m3riJM0ggqqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping particular indices\n",
        "\n",
        "df = df.drop(list_of_indices)\n",
        "df_dropped = df.drop(columns=['Age', 'Score'])"
      ],
      "metadata": {
        "id": "uiU0HBS6s1G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Categorical Featuress"
      ],
      "metadata": {
        "id": "CqPUDYCgtS-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### See how target variable 1 or 0 etc is ditributed among the categorical and other pivot tables to help check trends/importance/distributions\n",
        "\n",
        "# compare survival rate across Age, SibSp, Parch, and Fare -> see how higher fare survived more (numeric)\n",
        "pd.pivot_table(train_df2, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])\n",
        "\n",
        "# compare survival across Pclass, Sex, Embarked -> see how female/male works (non-numeric)\n",
        "\n",
        "print(pd.pivot_table(train_df2, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\n",
        "print()\n",
        "print(pd.pivot_table(train_df2, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\n",
        "print()\n",
        "print(pd.pivot_table(train_df2, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))"
      ],
      "metadata": {
        "id": "UAWl1NZxtSc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This simply plots bar graph where x-axis is the unique categories and y axis is the value counts\n",
        "\n",
        "for i in ['Survived','Pclass','Sex','Ticket','Cabin','Embarked']: ##categorical\n",
        "    sns.barplot(train_df2[i].value_counts().index,train_df2[i].value_counts()).set_title(i)\n",
        "    plt.show()\n",
        "\n",
        "#Cabin and ticket graphs are very messy. This is an area where we may want to do some feature engineering!"
      ],
      "metadata": {
        "id": "uMA7nuLAtXSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "g4Dkh82XxS2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Log transform: Look at historgram and then log histogram of target variables (use np.log1p)"
      ],
      "metadata": {
        "id": "Xf8HSG0gxxY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoding vs ordinal encoding for categorical features: see scatterplot of the categorical features vs y\n",
        "\n",
        "#One Hot Encoding\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, drop=None)\n",
        "# Transform the 'Color' column\n",
        "encoded = one_hot_encoder.fit_transform(data[['Color']])\n",
        "# Combine with original DataFrame\n",
        "data_one_hot = pd.DataFrame(encoded, columns=one_hot_encoder.get_feature_names_out(['Color']))\n",
        "data = pd.concat([data, data_one_hot], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "## Ordinal Encoding\n",
        "# Define the order for encoding\n",
        "education_order = ['High School', 'Bachelor', 'Master', 'PhD']\n",
        "ordinal_encoder = OrdinalEncoder(categories=[education_order])\n",
        "\n",
        "data['Education_Encoded'] = ordinal_encoder.fit_transform(data[['Education']])\n",
        "\n",
        "\n",
        "\n",
        "# Target encoding for categorical variables\n",
        "\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "\n",
        "encoder = TargetEncoder()\n",
        "df['Category_encoded'] = encoder.fit_transform(df['Category'], df['Target'])\n",
        "print(df)\n",
        "\n",
        "\n",
        "#Do it manually\n",
        "category_median = df.groupby('Category')['Target'].median()\n",
        "df['Category_encoded'] = df['Category'].map(category_median)\n"
      ],
      "metadata": {
        "id": "rivZ_t0-yjX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Proper Data\n",
        "\n",
        "def preprocess(data):\n",
        "\n",
        "    imputer1 = SimpleImputer(strategy='mean')\n",
        "    data[continuous_numerical_features] = imputer1.fit_transform(data[continuous_numerical_features])\n",
        "\n",
        "    imputer2 = SimpleImputer(strategy='constant', fill_value='Do_not_have_this_feature')\n",
        "    data = data.fillna(data[nominal_features+ ordinal_features].mode().iloc[0])\n",
        "\n",
        "    #One Hot Encoding\n",
        "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore', drop=None)\n",
        "    encoded = one_hot_encoder.fit_transform(data[nominal_features])\n",
        "    encoded_dense = encoded.toarray()\n",
        "    data_one_hot = pd.DataFrame(encoded_dense, columns=one_hot_encoder.get_feature_names_out(nominal_features), index=data.index)\n",
        "    data = pd.concat([data, data_one_hot], axis=1)\n",
        "    data = data.drop(columns=nominal_features, axis=1)\n",
        "\n",
        "    # Label Encoding for specific features\n",
        "    label_encoders = {}\n",
        "    for feature in label_encoding_features:\n",
        "        label_encoder = LabelEncoder()\n",
        "        data[feature] = label_encoder.fit_transform(data[feature])\n",
        "        label_encoders[feature] = label_encoder  # Save the encoder for potential inverse transformations later\n",
        "\n",
        "    #Ordinal encoding\n",
        "    ordinal_encoder = OrdinalEncoder()\n",
        "    data[ordinal_features] = ordinal_encoder.fit_transform(data[ordinal_features])\n",
        "\n",
        "    data = data[continuous_numerical_features + data_one_hot.columns.tolist() + ordinal_features + ['SalePrice']]\n",
        "\n",
        "    return data, imputer1, imputer2, one_hot_encoder, ordinal_encoder\n",
        "\n",
        "def process_test(data, imputer1, one_hot_encoder, ordinal_encoder ):\n",
        "    data[continuous_numerical_features] = imputer1.transform(data[continuous_numerical_features])\n",
        "    data = data.fillna(data[nominal_features+ ordinal_features].mode().iloc[0])\n",
        "    encoded = one_hot_encoder.transform(data[nominal_features])\n",
        "    encoded_dense = encoded.toarray()\n",
        "    data_one_hot = pd.DataFrame(encoded_dense, columns=one_hot_encoder.get_feature_names_out(nominal_features), index=data.index)\n",
        "    data = pd.concat([data, data_one_hot], axis=1)\n",
        "    data = data.drop(columns=nominal_features, axis=1)\n",
        "    data[ordinal_features] = ordinal_encoder.transform(data[ordinal_features])\n",
        "    data = data[continuous_numerical_features + data_one_hot.columns.tolist() + ordinal_features + ['SalePrice']]\n",
        "    return data"
      ],
      "metadata": {
        "id": "N1U-rb1tG23y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split in train and test data (completely unseen)\n",
        "train_df, test_df = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make X_train and y_train from processed data\n",
        "train_df_proc, imputer1, imputer2, one_hot_encoder, ordinal_encoder = preprocess(train_df)\n",
        "test_df_proc = process_test(test_df, imputer1, one_hot_encoder, ordinal_encoder)"
      ],
      "metadata": {
        "id": "S7Xny2OJgs4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time-Series"
      ],
      "metadata": {
        "id": "NfqZQV_0xbrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rolling mean based on time"
      ],
      "metadata": {
        "id": "93bXcRO_xM7T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfWKeI59wlgS",
        "outputId": "a64ebdd6-d2dd-4f1e-84e7-b334a3d07ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     value  rolling_mean\n",
            "timestamp                               \n",
            "2023-01-01 00:00:00   10.0          10.0\n",
            "2023-01-01 01:00:00   20.0          15.0\n",
            "2023-01-01 02:00:00    NaN          15.0\n",
            "2023-01-01 03:00:00   40.0          30.0\n",
            "2023-01-01 04:00:00   50.0          45.0\n",
            "2023-01-01 05:00:00   60.0          50.0\n",
            "2023-01-01 06:00:00   70.0          60.0\n",
            "2023-01-01 07:00:00   80.0          70.0\n",
            "2023-01-01 08:00:00   90.0          80.0\n",
            "2023-01-01 09:00:00  100.0          90.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-33f00e66a605>:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  'timestamp': pd.date_range(start='2023-01-01', periods=10, freq='H'),\n",
            "<ipython-input-2-33f00e66a605>:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  df['rolling_mean'] = df['value'].rolling('3H').mean()\n"
          ]
        }
      ],
      "source": [
        "# Set 'timestamp' as the index (optional, depends on your use case)\n",
        "df.set_index('timestamp', inplace=True)\n",
        "\n",
        "# Calculate rolling mean with a time-based window of 3 hours\n",
        "df['rolling_mean'] = df['value'].rolling('3H').mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Other time series snippets"
      ],
      "metadata": {
        "id": "F5jTTzhfKSiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Time Series Data!!\n",
        "\n",
        "###LAG BASED FEATURES - 28 DAY SHIFT ETC\n",
        "### ROLLING MEAN/STD FOR DIFFERENT PERIODS\n",
        "###MOMENTUM LIKE FEATURES EWMA ETC\n",
        "##DIFFERENT MODELS PER DAY/PER STORE\n",
        "## GROUP BY PER DAY/PER STORE ETC\n",
        "##DAYS SINCE LAST SOLD\n",
        "##LAST SOLD PRICE\n",
        "##PROMOTION CYCLES, DAYS SINCE PROMOTIONS\n",
        "## BY DAYS OF WEEK ETC\n",
        "##Differencing for trends versus means or difference vs last time or something\n",
        "\n",
        "melt_train[\"lag_sales_1\"] = melt_train.groupby(\"Product_Code\")['Sales'].shift(1)\n",
        "melt_train[\"diff_sales_1\"] = melt_train.groupby(\"Product_Code\")['Sales'].diff(1)\n",
        "melt_train.groupby(\"Product_Code\")['Sales'].rolling(4).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "\n",
        "df['hour'] = df.index.hour\n",
        "df['dayofweek'] = df.index.dayofweek\n",
        "df['quarter'] = df.index.quarter\n",
        "df['month'] = df.index.month\n",
        "df['year'] = df.index.year\n",
        "df['dayofyear'] = df.index.dayofyear\n",
        "df['dayofmonth'] = df.index.day\n",
        "df['weekofyear'] = df.index.isocalendar().week\n",
        "\n",
        "\n",
        "def add_lags(df):\n",
        "    target_map = df['PJME_MW'].to_dict()\n",
        "    df['lag1'] = (df.index - pd.Timedelta('364 days')).map(target_map)\n",
        "    df['lag2'] = (df.index - pd.Timedelta('728 days')).map(target_map)\n",
        "    df['lag3'] = (df.index - pd.Timedelta('1092 days')).map(target_map)\n",
        "    return df\n",
        "\n",
        "#Introduce lags\n",
        "lags = [1,2,3,6,12,24,36]\n",
        "for lag in lags:\n",
        "    df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)\n",
        "\n",
        "\n",
        "# Calculate log returns and pct returns\n",
        "for col in data.columns:\n",
        "    # Log returns\n",
        "    data[f'{col}_log_return'] = np.log(data[col] / data[col].shift(1))\n",
        "    # Percentage returns\n",
        "    data[f'{col}_pct_return'] = data[col].pct_change()\n",
        "\n",
        "# WARNING: Has future info\n",
        "df['iteam_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)\n",
        "df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)\n",
        "df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)\n",
        "df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\n",
        "df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\n",
        "df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\n",
        "df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
        "df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
        "df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\n",
        "df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)\n",
        "df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)\n",
        "df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\n",
        "\n",
        "\n",
        "df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
        "df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean())"
      ],
      "metadata": {
        "id": "H8-mLXlXKaUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Time Series Cross Validation: roll forward\n",
        "\n",
        "tss = TimeSeriesSplit(n_splits=5, test_size=24*365*1, gap=24) #Can also add a max_train_size arg so that it becomes rolling after a while\n",
        "\n",
        "df = df.sort_index()\n",
        "\n",
        "\n",
        "fold = 0\n",
        "preds = []\n",
        "scores = []\n",
        "for train_idx, val_idx in tss.split(df):\n",
        "    train = df.iloc[train_idx]\n",
        "    test = df.iloc[val_idx]\n",
        "\n",
        "    train = create_features(train)\n",
        "    test = create_features(test)\n",
        "\n",
        "    FEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month','year',\n",
        "                'lag1','lag2','lag3']\n",
        "    TARGET = 'PJME_MW'\n",
        "\n",
        "    X_train = train[FEATURES]\n",
        "    y_train = train[TARGET]\n",
        "\n",
        "    X_test = test[FEATURES]\n",
        "    y_test = test[TARGET]\n",
        "\n",
        "    reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',\n",
        "                           n_estimators=1000,\n",
        "                           early_stopping_rounds=50,\n",
        "                           objective='reg:linear',\n",
        "                           max_depth=3,\n",
        "                           learning_rate=0.01)\n",
        "    reg.fit(X_train, y_train,\n",
        "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "            verbose=100)\n",
        "\n",
        "    y_pred = reg.predict(X_test)\n",
        "    preds.append(y_pred)\n",
        "    score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    scores.append(score)"
      ],
      "metadata": {
        "id": "88FpyHu7K-Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Processing Pipeline"
      ],
      "metadata": {
        "id": "tyz3OIZf8Ijz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pipelining\n",
        "\n",
        "num_pipeline = Pipeline(steps=[\n",
        "    ('impute', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "ode_pipeline = Pipeline(steps=[\n",
        "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ode', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "\n",
        "ohe_pipeline = Pipeline(steps=[\n",
        "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "col_trans = ColumnTransformer(transformers=[\n",
        "    ('num_p', num_pipeline, num_cols),\n",
        "    ('ode_p', ode_pipeline, ode_cols),\n",
        "    ('ohe_p', ohe_pipeline, ohe_cols),\n",
        "    ],\n",
        "    remainder='passthrough')\n",
        "\n",
        "#remainder = drop if we want to drop\n",
        "\n",
        "\n",
        "pipeline = Pipeline(steps = [\n",
        "    ('preprocessing', col_trans)\n",
        "])\n",
        "\n",
        "X = train_df.drop('target_var')\n",
        "y = train_df('target_var')\n",
        "\n",
        "x_preprocessed = pipeline.fit_transform(X)"
      ],
      "metadata": {
        "id": "zHRpMRBR8K3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PIPELINE WITH GRIDSEARCH\n",
        "\n",
        "# Sort by timestamp (important for time series data)\n",
        "data = data.sort_values(by='Timestamp').reset_index(drop=True)\n",
        "\n",
        "# Define features and target\n",
        "X = data.drop(columns=['Target', 'Timestamp'])\n",
        "y = data['Target']\n",
        "\n",
        "# Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Define column groups\n",
        "numerical_features = ['Numerical1', 'Numerical2']\n",
        "categorical_features_onehot = ['Categorical']  # For one-hot encoding\n",
        "categorical_features_target = ['Categorical']  # For target encoding\n",
        "\n",
        "# Define preprocessing for each column group\n",
        "numerical_transformer = StandardScaler()\n",
        "onehot_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "target_transformer = TargetEncoder()\n",
        "\n",
        "# Combine preprocessors in a column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('onehot', onehot_transformer, categorical_features_onehot),\n",
        "        ('target', target_transformer, categorical_features_target)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100],\n",
        "    'classifier__max_depth': [5, 10],\n",
        "    'classifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# TimeSeriesSplit for cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# GridSearchCV with TimeSeriesSplit\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=tscv,            # TimeSeriesSplit ensures no look-ahead bias\n",
        "    scoring='accuracy', # Metric to optimize\n",
        "    n_jobs=-1,          # Use all available CPU cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Perform grid search on the train set\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and cross-validation score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Wj41hRmVe0bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of principal components\n",
        "n_components = 5\n",
        "\n",
        "# Define the pipeline: Standardization -> PCA -> Regression\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),             # Standardize features\n",
        "    ('pca', PCA(n_components=n_components)),  # Reduce dimensionality\n",
        "    ('regressor', LinearRegression())         # Perform regression\n",
        "])\n",
        "\n",
        "# Define K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(pipeline, X, y, cv=kf, scoring='r2')  # Using R² as the metric\n",
        "\n",
        "# Display the results\n",
        "print(f\"Cross-validated R² scores: {cv_scores}\")\n",
        "print(f\"Mean R² score: {cv_scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "TadvMpCLOT7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "education_mapping = {\n",
        "    'High School': 0,\n",
        "    'Bachelor': 1,\n",
        "    'Master': 2,\n",
        "    'PhD': 3\n",
        "}\n",
        "\n",
        "satisfaction_mapping = {\n",
        "    'Low': 0,\n",
        "    'Medium': 1,\n",
        "    'High': 2\n",
        "}\n",
        "\n",
        "# Apply mappings\n",
        "data['Education_Encoded'] = data['Education'].map(education_mapping)\n",
        "data['Satisfaction_Encoded'] = data['Satisfaction'].map(satisfaction_mapping)"
      ],
      "metadata": {
        "id": "cDT5HzYp-Yfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_preprocessed, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "C3fK_7Zm8TQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "gTzA-QTNHRfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'RandomForest': RandomForestRegressor(random_state=42),\n",
        "    'XGBoost': XGBRegressor(random_state=42),\n",
        "    'ridge': Ridge(),\n",
        "    'GradBoost':GradientBoostingRegressor(),\n",
        "    'lgbm' : lgb.LGBMRegressor(),\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'LinearRegression': {},\n",
        "\n",
        "    'RandomForest': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [None, 5, 10, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "    },\n",
        "\n",
        "    'XGBoost': {\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'n_estimators': [200, 300],\n",
        "        'max_depth': [3, 5],\n",
        "        'min_child_weight': [1, 2],\n",
        "        'gamma': [0, 0.1],\n",
        "        'subsample': [0.8, 0.9],\n",
        "        'colsample_bytree': [0.8, 0.9],\n",
        "    },\n",
        "\n",
        "    'ridge': {\n",
        "        'alpha': [0.05, 0.1, 1, 5, 10],\n",
        "        'solver': ['auto', 'svd', 'sparse_cg'],\n",
        "    },\n",
        "\n",
        "    'GradBoost': {\n",
        "        'max_depth': [6, 10, 15],\n",
        "        'n_estimators': [200, 300],\n",
        "        'min_samples_leaf': [10, 25],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'max_features': ['sqrt', 'log2'],\n",
        "    },\n",
        "\n",
        "    'lgbm': {\n",
        "        'boosting_type': ['gbdt', 'dart'],\n",
        "        'num_leaves': [20, 30, 40],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'n_estimators': [100, 200],\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3-fold cross-validation\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train and tune the models\n",
        "grids = {}\n",
        "best_estimator = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f'Training and tuning {model_name}...')\n",
        "    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', verbose=2)\n",
        "    grids[model_name].fit(X_train, y_train)\n",
        "    best_params = grids[model_name].best_params_\n",
        "    best_estimator[model_name] = grids[model_name].best_estimator_\n",
        "    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n",
        "\n",
        "    print(f'Best parameters for {model_name}: {best_params}')\n",
        "    print(f'Best RMSE for {model_name}: {best_score}\\n')\n"
      ],
      "metadata": {
        "id": "DuNpjzMDHWFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Voting Ensemble Regressors\n",
        "vr = VotingRegressor([('lgbm', best_estimator['lgbm']),\n",
        "                      ('GradBoost', best_estimator['GradBoost']),\n",
        "                      ('ridge', best_estimator['ridge'])],\n",
        "                    weights=[3,2,1]) ##hard and soft voting\n",
        "\n",
        "vr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_vr = vr.predict(X_test)"
      ],
      "metadata": {
        "id": "XlqR3sZdHeI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'SVC': SVC(probability=True),\n",
        "    'XGBoost': XGBClassifier(random_state=1),\n",
        "    'RandomForest': RandomForestClassifier(random_state=1),\n",
        "    'LightGBM': lgb.LGBMClassifier(random_state=1)\n",
        "\n",
        "}\n",
        "\n",
        "# Define the hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    'LogisticRegression': {\n",
        "        'max_iter': [1000],  # 1000 is usually enough; 2000 may be overkill\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'C': np.logspace(-3, 3, 7),  # Reduce range for efficiency; 7 values are sufficient\n",
        "        'solver': ['liblinear'],  # Required for L1 and L2 penalties\n",
        "    },\n",
        "\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5, 7],  # Focused range for efficient tuning\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'algorithm': ['auto'],  # Simplify to 'auto' for general use\n",
        "        'p': [1, 2],  # Manhattan (p=1) or Euclidean (p=2) distances\n",
        "    },\n",
        "\n",
        "    'SVC': {\n",
        "        'kernel': ['rbf', 'linear', 'poly'],  # Consider the 3 main kernel types\n",
        "        'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
        "        'gamma': ['scale', 'auto'],  # For 'rbf' and 'poly' kernels\n",
        "        'degree': [2, 3],  # Relevant only for 'poly' kernel\n",
        "    },\n",
        "\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [100, 200],  # Reduced for efficiency\n",
        "        'colsample_bytree': [0.8, 0.9],\n",
        "        'max_depth': [3, 5],  # Common values for max depth\n",
        "        'reg_alpha': [0.1, 1],  # L1 regularization\n",
        "        'reg_lambda': [1, 2],  # L2 regularization\n",
        "        'subsample': [0.8, 1.0],\n",
        "        'learning_rate': [0.05, 0.1],  # Reduced to a reasonable range\n",
        "        'gamma': [0, 0.1, 0.2],\n",
        "        'min_child_weight': [1, 2],\n",
        "    },\n",
        "\n",
        "    'RandomForest': {\n",
        "        'n_estimators': [100, 200, 300],  # Commonly used values\n",
        "        'criterion': ['gini', 'entropy'],  # Impurity measures\n",
        "        'bootstrap': [True],  # Standard setting for Random Forest\n",
        "        'max_depth': [10, 20],  # Depth of the trees\n",
        "        'max_features': ['auto', 'sqrt'],  # 'auto' = all features; 'sqrt' = sqrt of features\n",
        "        'min_samples_leaf': [1, 2],  # Small leaf sizes for flexibility\n",
        "        'min_samples_split': [2, 5],  # Standard splitting options\n",
        "    },\n",
        "     'LightGBM':  {\n",
        "      'n_estimators': [100, 200],           # Small range for boosting rounds\n",
        "      'max_depth': [-1, 5],                # No limit (-1) and a moderate value\n",
        "      'learning_rate': [0.05, 0.1],        # Common learning rates\n",
        "      'subsample': [0.8, 1.0],             # Slight row sampling\n",
        "      'colsample_bytree': [0.8, 1.0],      # Slight feature sampling\n",
        "      'num_leaves': [31, 50],              # Default (31) and slightly larger\n",
        "      'reg_lambda': [0, 1.0],              # L2 regularization for overfitting\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# 3-fold cross-validation\n",
        "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Train and tune the models\n",
        "grids = {}\n",
        "best_estimator = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f'Training and tuning {model_name}...')\n",
        "    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='f1', verbose=2)\n",
        "    grids[model_name].fit(X_train, y_train)\n",
        "    best_params = grids[model_name].best_params_\n",
        "    best_estimator[model_name] = grids[model_name].best_estimator_\n",
        "    best_score = grids[model_name].best_score_\n",
        "\n",
        "    print(f'Best parameters for {model_name}: {best_params}')\n",
        "    print(f'Best F1-score for {model_name}: {best_score}\\n')\n"
      ],
      "metadata": {
        "id": "Ob2XJjo7Hj2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "vc = VotingClassifier([('rfr', best_estimator['rfr']),\n",
        "                      ('xgb', best_estimator['XGBoost']),\n",
        "                      ('logistic', best_estimator['LogisticRegression'])],\n",
        "                    weights=[2,3,1], voting = 'soft')\n",
        "vc.fit(X_train, y_train)\n",
        "y_pred_vc = vc.predict(X_test)\n",
        "f1_score(y_test, y_pred_vc, average='weighted')\n",
        "\n"
      ],
      "metadata": {
        "id": "PHDeGokOHqUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_cross_validation(df, model, window_size, val_size, test_size, evaluate_model):\n",
        "    dates = df.dates.unique()\n",
        "    n = len(dates)\n",
        "\n",
        "    # List to store validation scores\n",
        "    val_scores = []\n",
        "\n",
        "    # Start from window_size to ensure the training data can extend backward\n",
        "    for i in range(window_size, n - val_size - test_size + 1, val_size):\n",
        "        # Define training and validation windows\n",
        "        dates_train = dates[i-window_size:i]  # Training window (past `window_size` dates)\n",
        "        dates_val = dates[i:i+val_size]      # Validation window (next `val_size` dates)\n",
        "\n",
        "        # Filter data for the respective sets\n",
        "        train_data = df[df['dates'].isin(dates_train)]\n",
        "        val_data = df[df['dates'].isin(dates_val)]\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        model.fit(train_data.drop(columns=['target']), train_data['target'])\n",
        "        val_predictions = model.predict(val_data.drop(columns=['target']))\n",
        "        val_score = evaluate_model(val_predictions, val_data['target'])\n",
        "        val_scores.append(val_score)\n",
        "\n",
        "        print(f\"Window ending at index {i}: Validation Score: {val_score}\")\n",
        "\n",
        "    # Compute the average validation score\n",
        "    average_val_score = np.mean(val_scores)\n",
        "    print(f\"Average Validation Score across all splits: {average_val_score}\")\n",
        "\n",
        "    return average_val_score, val_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "x4X9olJov3ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_test_validation(df, model, window_size, val_size, test_size, evaluate_model):\n",
        "    dates = df.dates.unique()\n",
        "    n = len(dates)\n",
        "\n",
        "    # List to store validation scores and predictions\n",
        "    val_scores = []\n",
        "    all_predictions = []\n",
        "\n",
        "    # Start from window_size to ensure the training data can extend backward\n",
        "    for i in range(n - test_size, n - val_size - 1, val_size):\n",
        "        # Define training and validation windows\n",
        "        dates_train = dates[i-window_size:i]  # Training window (past `window_size` dates)\n",
        "        dates_val = dates[i:i+val_size]      # Validation window (next `val_size` dates)\n",
        "\n",
        "        # Filter data for the respective sets\n",
        "        train_data = df[df['dates'].isin(dates_train)]\n",
        "        val_data = df[df['dates'].isin(dates_val)]\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        model.fit(train_data.drop(columns=['target']), train_data['target'])\n",
        "        val_predictions = model.predict(val_data.drop(columns=['target']))\n",
        "        val_score = evaluate_model(val_predictions, val_data['target'])\n",
        "\n",
        "        # Append validation score and predictions\n",
        "        val_scores.append(val_score)\n",
        "        all_predictions.extend(val_predictions)  # Flatten the predictions\n",
        "\n",
        "        print(f\"Window ending at index {i}: Validation Score: {val_score}\")\n",
        "\n",
        "    # Compute the average validation score\n",
        "    average_val_score = np.mean(val_scores)\n",
        "    print(f\"Average Validation Score across all splits: {average_val_score}\")\n",
        "\n",
        "    return average_val_score, val_scores, all_predictions\n"
      ],
      "metadata": {
        "id": "PkYO_svHzX8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TimeSeriesSplit for cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Train and tune the models\n",
        "grids = {}\n",
        "best_estimator = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f'Training and tuning {model_name}...')\n",
        "    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=tscv, scoring='neg_mean_squared_error', verbose=2)\n",
        "    grids[model_name].fit(X_train, y_train)\n",
        "    best_params = grids[model_name].best_params_\n",
        "    best_estimator[model_name] = grids[model_name].best_estimator_\n",
        "    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n",
        "\n",
        "    print(f'Best parameters for {model_name}: {best_params}')\n",
        "    print(f'Best RMSE for {model_name}: {best_score}\\n')\n"
      ],
      "metadata": {
        "id": "TPyw9O3H0tod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Time Series Cross Validation\n",
        "\n",
        "tss = TimeSeriesSplit(n_splits=5, test_size=24*365*1, gap=24)\n",
        "df = df.sort_index()\n",
        "\n",
        "\n",
        "fold = 0\n",
        "preds = []\n",
        "scores = []\n",
        "for train_idx, val_idx in tss.split(df):\n",
        "    train = df.iloc[train_idx]\n",
        "    test = df.iloc[val_idx]\n",
        "\n",
        "    train = create_features(train)\n",
        "    test = create_features(test)\n",
        "\n",
        "    FEATURES = ['dayofyear', 'hour', 'dayofweek', 'quarter', 'month','year',\n",
        "                'lag1','lag2','lag3']\n",
        "    TARGET = 'PJME_MW'\n",
        "\n",
        "    X_train = train[FEATURES]\n",
        "    y_train = train[TARGET]\n",
        "\n",
        "    X_test = test[FEATURES]\n",
        "    y_test = test[TARGET]\n",
        "\n",
        "    reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',\n",
        "                           n_estimators=1000,\n",
        "                           early_stopping_rounds=50,\n",
        "                           objective='reg:linear',\n",
        "                           max_depth=3,\n",
        "                           learning_rate=0.01)\n",
        "    reg.fit(X_train, y_train,\n",
        "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "            verbose=100)\n",
        "\n",
        "    y_pred = reg.predict(X_test)\n",
        "    preds.append(y_pred)\n",
        "    score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    scores.append(score)"
      ],
      "metadata": {
        "id": "klUcPKjj01EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "ANm82U6rez-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred_vc)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=vc.classes_, yticklabels=vc.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OlL2hakPaTsr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}